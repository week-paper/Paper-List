# Paper List
> **[“游戏规则”(Our Terms)](https://github.com/Week-Paper/Paper-List/issues/3)**

## 第一周
- ~~张路~~
  - ~~[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)~~
- 于军帅
  - [Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution](https://export.arxiv.org/pdf/1904.05049)
- 杨德杰
  - [Training Classifiers with Natural Language Explanations](https://aclweb.org/anthology/P18-1175)
- 李哲舟
  - [RePr: Improved Training of Convolutional Filters](https://arxiv.org/pdf/1811.07275v3.pdf)  
- 张家瑞
  - [Phrase-Based & Neural Unsupervised Machine Translation](https://arxiv.org/abs/1804.07755)
- 李亮
  - [Disconnected Recurrent Neural Networks for Text Categorization](https://www.aclweb.org/anthology/P18-1215)
  
## 第二周
- 于军帅
  - [Star Transformers](https://arxiv.org/abs/1902.09113)
- 杨德杰
  - [Reading Wikipedia to Answer Open-Domain Questions](https://cs.stanford.edu/~danqi/papers/acl2017.pdf)
- 张家瑞
  - [Competence-based Curriculum Learning for Neural Machine Translation](https://arxiv.org/abs/1903.09848)
- 李亮
  - [Selective Encoding for Abstractive Sentence Summarization](https://arxiv.org/abs/1704.07073)
- 李哲舟
  - [Learning without Forgetting](https://arxiv.org/pdf/1606.09282.pdf)

## 第三周

- 杨德杰
  - [CoQA: A Conversational Question Answering Challenge](https://arxiv.org/pdf/1808.07042.pdf)
- 李亮
  - [A Deep Reinforced Model for Abstractive Summarization](https://arxiv.org/pdf/1705.04304.pdf)
- 李哲舟
  - [Chat More: Deepening and Widening the Chatting Topic via A Deep Model](http://coai.cs.tsinghua.edu.cn/hml/media/files/2018SIGIR_Wangwenjie.pdf)  
- 张家瑞
  - [An Attentive Survey of Attention Models](https://arxiv.org/abs/1904.02874?context=cs)
## 第四周
- 杨德杰
  - [Multimodal Affective Analysis Using Hierarchical Attention Strategy with Word-Level Alignment](https://www.aclweb.org/anthology/P18-1207)
- 张家瑞
  - [Learning a Wavelet-like Auto-Encoder to Accelerate Deep Neural Networks](https://arxiv.org/abs/1712.07493)
## 第五周
 - 杨德杰
   - [An Attentive Survey of Attention Models](https://arxiv.org/pdf/1904.02874.pdf)
   - [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
   - [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)
 - 张家瑞
   - [Ordered Neurons Integrating Tree Structures into Recurrent Neural Networks](https://arxiv.org/abs/1810.09536?context=cs)
## 第六周
 - 杨德杰
   - [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/pdf/1905.11946.pdf)

